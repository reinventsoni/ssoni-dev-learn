---
title: "Automating Repetitive Tasks: Productivity Hacks for Developers"
description: "Two Forms of Pre Rendering"
image: "https://raw.githubusercontent.com/reinventsoni/ssoni-dev-learn-blog/main/_images/Python.jpg"
publishedAt: "2023-01-01"
updatedAt: "2023-01-01"
author: "Sanjay Soni"
isPublished: "true"
tags: ["python", "development"]
---

```python
import numpy as np
```

```python
np.__version__
```

    '1.24.3'

```python
class Perceptron:
    """
    Perceptron Classifier

    Parameters
    ----------
    eta: float
        Learning Rate between 0.0 and 1.0
    n_iter: int
        Number of Iterations over the Training Set
    random_state: int
        Random Number Generator seed for random weight initialization

    Attributes:
    ----------
    w_: 1d-array
        Weights after fitting
    b_: Scalar
        Bias unit after fitting
    errors_: list
        Number of misclassification updates in each epoch

    """
    def __init__(self, eta=0.01, n_iter=50, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state


    def fit(self,X,y):
        """
        Fit Training Data

        Parameters:
        ----------
        X: {array -like} shape=[n_examples, n_features]
            Training Vectors, where n_examples is the number of examples,
            and n_features is the number of features
        y: {array_like} shape =[n_examples]
            Target Value corresponding to each training examples

        Returns:
        ----------
        self: object
        """

        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1])
        self.b_ = np.float_(0.)
        self.errors_ = []

        for _ in range(self.n_iter):
            errors = 0
            for xi, target in zip(X,y):
                update = self.eta * (target - self.predict(xi))
                self.w_ += update * xi
                self.b_ += update
                errors += int(update !=0.0)
            self.errors_.append(errors)

        return self

    def net_input(self,X):
        """ Calculate net input"""
        return np.dot(X, self.w_) + self.b_

    def predict(self, X):
        return np.where(self.net_input(X) >=0.0, 1, 0)

```

```python
import os
import pandas as pd
```

```python
#IRIS Dataset
s= 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'
print(f'From URL: {s}')
```

    From URL: https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data

```python
df=pd.read_csv(s, header=None, encoding='utf-8')
df.tail()
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }

</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>145</th>
      <td>6.7</td>
      <td>3.0</td>
      <td>5.2</td>
      <td>2.3</td>
      <td>Iris-virginica</td>
    </tr>
    <tr>
      <th>146</th>
      <td>6.3</td>
      <td>2.5</td>
      <td>5.0</td>
      <td>1.9</td>
      <td>Iris-virginica</td>
    </tr>
    <tr>
      <th>147</th>
      <td>6.5</td>
      <td>3.0</td>
      <td>5.2</td>
      <td>2.0</td>
      <td>Iris-virginica</td>
    </tr>
    <tr>
      <th>148</th>
      <td>6.2</td>
      <td>3.4</td>
      <td>5.4</td>
      <td>2.3</td>
      <td>Iris-virginica</td>
    </tr>
    <tr>
      <th>149</th>
      <td>5.9</td>
      <td>3.0</td>
      <td>5.1</td>
      <td>1.8</td>
      <td>Iris-virginica</td>
    </tr>
  </tbody>
</table>
</div>

We extract the first 100 class labels that correspond to the 50 Iris-setosa and 50 Iris-versicolor flower and convert the class labels into the two integer class labels 1(versicolor) and 0 (setosa), that we assign to a vector y, where the values method of a pandas DataFrame yields the corresponding NumPy Representations

Similarly we will extract the first feature column (sepal length) and third feature column (petal length) of those 100 training examples and assigm them to feature a matrix X, which we can visualize via a two dimensional scatter plot

```python
import matplotlib.pyplot as plt
import numpy as np
# Select Setosa and Versicolor
y = df.iloc[0:100, 4].values
y = np.where(y=="Iris-setosa", 0, 1)
y.shape
```

    (100,)

```python
X = df.iloc[0:100, [0,2]].values
X.shape
```

    (100, 2)

```python
#Plot Data
plt.scatter(X[:50,0], X[:50,1], color='red', marker='o', label='Setosa')
plt.scatter(X[50:,0], X[50:,1], color='blue', marker='x', label='Versicolor')
plt.xlabel('Sepal Length (cm)')
plt.ylabel('Petal Length (cm)')
plt.legend(loc='upper left')
plt.show()
```

![png](output_9_0.png)

Figure above shows the distribution of the flower examples in the Iris dataset along the two feature axes: Petal Length and Sepal Length. In this two-dimensional feature subspace; we can see that a Linear Decision boundary should be sufficient to seprate Setosa from Versicolor flowers. Thus a linear classifier like Perceptron should be able to classify the flowers in this dataset perfectly.

We will now train our Perceptron Algorithm no the IRIS data subset that we just extracted. Also we will plot the misclassification error for each of the epoch to check whether the algorith converged and found a decision boundary that separates the two Iris Flower classes or not.

```python
ppn = Perceptron(eta=0.1, n_iter=10)
ppn.fit(X,y)
plt.plot(range(1,len(ppn.errors_)+1), ppn.errors_, marker='o')
```

    [<matplotlib.lines.Line2D at 0x23b4ccf1b90>]

![png](output_11_1.png)

As we can see our Perceptron converged after the 6th epoch and should now be able to classify the training examples perfectly. Let's implement the small convenience function to visualize the decision boudaries for the two-dimensional datasets

```python
from matplotlib.colors import ListedColormap
def plot_decision_regions(X,y,classifier,resolution=0.02):
    #setup marker generator and color map
    markers = ('o','s','^', 'v','<')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])

    #plot the decision surface
    x1_min, x1_max = X[:,0].min() - 1, X[:,0].max() + 1
    x2_min, x2_max = X[:,1].min() - 1, X[:,1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                          np.arange(x2_min, x2_max, resolution))
    lab = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    lab = lab.reshape(xx1.shape)
    plt.contourf(xx1, xx2, lab, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())

    #plot class examples
    for idx, cl in enumerate(np.unique(y)):
        plt.scatter(x=X[y==cl, 0],
                   y=X[y==cl, 1],
                   alpha=0.8,
                   c=colors[idx],
                   marker = markers[idx],
                   label=f'Class {cl}',
                   edgecolor='black')
```

```python
plot_decision_regions(X,y,classifier=ppn)
plt.xlabel('Sepal Length [cm]')
plt.ylabel('Petal Length [cm]')
plt.legend(loc='upper left')
plt.show()
```

![png](output_14_0.png)

As we can see in the plot, the perceptron learned a decision boundary that can classify all flower examples in the Iris training subset perfectly.

Although, in this particular case this algorithm converged, but convergence is one of the biggest problem of the Perceptron algorithm.

## Adaline Algorithm

In this section we will look at another type of single layer neural network algorithm ADAptive LInear NEuron (ADALINE). Adaline algorithm was published few years after that Perceptron Algorithm, by Bernard Widrow and his doctoral student Tedd Hoff, and it can be considered as an improvement to original perceptron algorithm

```python
class AdalineGD:
    """
    Adaptive Linear Neuron Classifier

    Parameters:
    -----------
    eta: float
        Learning Rate between 0.0 and 1.0

    n_iter: int
        Passes over the Training Set

    random_state:int
        Random Number Generator seed for Random Weight Initialization


    Attributes:
    -----------
    w_ : 1d-array
        Weights after fitting

    b_ : Scalar
        Bias unit after fitting

    losses: list
        Mean Squared Error Loss function values in each epoch

    """

    def __init__(self, eta=0.01, n_iter=50, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state

    def fit(self,X,y):
        """Fit Training Data

        Parameters:
        ----------
        X: {array-like}, shape=[n_examples, n_features]
            Training Vector where n_examples is the number of examples
            n_features is the number of features
        y: {array-like}, shape = [n_examples]

        Return:
        -------
        self: object
        """
        rgen=np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1])
        self.b_ = np.float_(0.)
        self.losses_ = []

        for i in range(self.n_iter):
            net_input = self.net_input(X)
            output = self.activation(net_input)
            errors = (y - output)

            self.w_ = self.eta * 2.0 * X.T.dot(errors) / X.shape[0]
            self.b_ = self.eta * 2.0 * errors.mean()
            loss = (errors**2).mean()
            self.losses_.append(loss)

        return self

    def net_input(self,X):
        """
        Calculate Net Input
        """
        return np.dot(X,self.w_) + self.b_

    def activation(self, X):
        """
        Compute Linear Activation
        """
        return X

    def predict(self,X):
        return np.where(self.activation(self.net_input(X))>=0.5, 1, 0)


```

```python
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,4))
ada1 = AdalineGD(n_iter=15, eta=0.1).fit(X,y)
ax[0].plot(range(1,len(ada1.losses_)+1),
          np.log10(ada1.losses_), marker='o')
ax[0].set_xlabel('Epochs')
ax[0].set_ylabel('log(Mean Squared Error)')
ax[0].set_title('Adaline Learning Rate 0.1')

ada2 = AdalineGD(n_iter=15, eta=0.01).fit(X,y)
ax[1].plot(range(1,len(ada2.losses_)+1), ada2.losses_, marker='o')
ax[1].set_xlabel('Epochs')
ax[1].set_ylabel('Mean Squared Error')
ax[1].set_title('Adaline Learning Rate 0.0001')
plt.show()
```

![png](output_18_0.png)

```python

```

```python

```
