---
title: "Artificial Neurons"
description: "The fundamental concept of Artificial Neuron as proposed by McCulloch Pitts Neuron"
image: "https://raw.githubusercontent.com/reinventsoni/ssoni-dev-learn-blog/main/_images/DeepLearning/_images/DeepLearningFoundations.webp"
publishedAt: "2024-10-04"
updatedAt: "2024-10-05"
author: "Sanjay Soni"
isPublished: "true"
tags: ["deep-learning", "deep-learning-foundations"]
---

# The First Artificial Neuron

## 1. Historical Roots of Deep Learning

Deep Learning has its root in 1940's, when **Warren McCulloch** and **Warren Pitts** published the concept of Artificial Neuron, also referred to as **MCP Neuron** in 1943 (A Logical Calculus of the Ideas Immanent in Nervous Activity by W.S McCulloch and W.Pitts, Bulletin of Mathetmatical Biophysics, 5(4): 115-133, 193)
[Reference](https://www.historyofinformation.com/detail.php?id=634)
and [Link to Paper](http://www.cse.chalmers.se/~coquand/AUTOMATA/mcp.pdf)

They drew on the three sources to come up with the concept of Artificial Neurons:

- knowledge of basic psychology and functions of neuron brain
- a formal analysis of propositional logic by **Russell** and **Whitehead**, and
- Turing's **Theory of Computation**

### 1.1 McCulloch Pitt's Neuron

The concept of artificial neuron was inspired by the **Biological Neurons** which are interconnected nerve cells in the brain and are involved in the processing and transmitting of various signals, which is illustrated in figure below:

<MDXImage
  src="https://raw.githubusercontent.com/reinventsoni/ssoni-dev-learn-blog/main/_images/DeepLearning/_images/BiologicalNeuron.webp"
  alt="Bilogoical Neuron"
  align="center"
  width="500"
  height="500"
/>

A single neuron in its simplified explanation consists of following parts:

- **SOMA**: the main part of the neuron which processes signal.
- **DENDRITES**: the branch-like shapes which receives signals from other neurons (i.e. are input to neuron), and
- **AXON**: a single nerve which sends signals to other neurons (i.e., is an output of the neuron)

and the connections between the nerurons are known as **SYNAPSES**

The idea of the **McCulloch Pitts Neuron** was to provide the abstractions on how the brain neuron works, and was considered as a simple logic gate which receives

- multiple input binary signals (equivalent to dendrites),
- binary output i.e. **ON** or **OFF** state (i.e. output of neuron, equivalent to Axon),
- and the neruron fires if the accumulates signal of all the inputs have enough stimulation in it, or in other words is above certain threshold.

#### 1.1.1 Formal Mathematical Definition of McCulloch Pitt's Neuron

- The inputs to MCP Neuron can be considered as a vector: **x**: [x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>, ....... x<sub>n-1</sub>, x<sub>n</sub>]
- Every input would have certain weights (importance) associated with it and let that be represented by the vector of weights **w**: [w<sub>1</sub>, w<sub>2</sub>, w<sub>3</sub>, ....... w<sub>n-1</sub>, w<sub>n</sub>], where each **w<sub>i</sub>** has a value of -1, 0 or 1
  - input signal with weight **1** are called excitory input, since they contribute towards a positive output signal in the sum
  - input signal with weight of **-1** are called inhibitory since they repress a positive output signal in the sum
  - input signal with weight of **0** do not contribute at all to the neuron
- The accumulated signal (sometimes also referred to as **net input**) can then be calculated as follows:
  - z = w<sub>1</sub>.x<sub>1</sub> + w<sub>2</sub>.x<sub>2</sub> + .... + w<sub>n-1</sub>.x<sub>n-1</sub> + w<sub>n</sub>.x<sub>n</sub>
- Then for some threshold value **t**, an integer, the output signal is determined by the **decision function** or **activation function** as follows:
  - y = 1, if z >= t or 0 otherwise

The neuron is said to be activated or in **ON** state (i.e. having value of 1), when the weighted sum is greater than the threshold value

<MDXImage
  src="https://raw.githubusercontent.com/reinventsoni/ssoni-dev-learn-blog/main/_images/DeepLearning/_images/BiologicalvsAritificalNeuron.png"
  alt="Biological Neuron vs Aritificial Neuron"
  align="center"
  width="1200"
  height="500"
/>

#### 1.1.2 Original Experiments using McCulloch Pitt's Neuron

The original experiments proposed by **Warren McCulloch** and **Warren Pitts** to use this model of artificial neurons, and construct different **LOGIC GATES** by simply specifying what the **weights** and **threshold** value should be. It can be easily shown that **MCP Neuron** can be used to model the **AND**, **OR** and **NOT** logic gates, as well as the composition of these three logic gates

**OR Gate**<br/>
OR Gate is a logic gate which returns true (1) if atleast one of the input signal is true i.e. 1. This can be achieved by setting

- Weights **w<sub>1</sub>** and **w<sub>1</sub>** to 1, and
- Threshold **t** = 1
  and we can easily see that, whenever the aggregated sum is greater then or equal to threshold t, the output is ON (as expected from OR operation)

| x<sub>1</sub> | x<sub>2</sub> | OR  | =>  | Agg. Sum      | is >= Threshold (1) | Output |
| ------------- | ------------- | --- | --- | ------------- | ------------------- | ------ |
| 0             | 0             | 0   | =>  | 1(0)+1(0) = 0 | No                  | 0      |
| 0             | 1             | 1   | =>  | 1(0)+1(1) = 1 | Yes                 | 1      |
| 1             | 0             | 1   | =>  | 1(1)+1(0) = 1 | Yes                 | 1      |
| 1             | 1             | 1   | =>  | 1(1)+1(1) = 1 | Yes                 | 1      |

**AND Gate**<br/>
AND Gate is a logic gate which returns true (1) if atleast one of the input signal is true i.e. 1. This can be achieved by setting

- Weights **w<sub>1</sub>** and **w<sub>1</sub>** to 1, and
- Threshold **t** = 2
  and we can easily see that, whenever the aggregated sum is greater then or equal to threshold t, the output is ON (as expected from OR operation)
  | x<sub>1</sub> | x<sub>2</sub> | OR | => | Agg. Sum | is >= Threshold (2) | Output |
  | ------------- | ------------- | -- |----| --------------| ------------------ | -------- |
  | 0 | 0 | 0 | => | 1(0)+1(0) = 0 | No | 0 |
  | 0 | 1 | 0 | => | 1(0)+1(1) = 1 | No | 0 |
  | 1 | 0 | 0 | => | 1(1)+1(0) = 1 | No | 0 |
  | 1 | 1 | 1 | => | 1(1)+1(1) = 2 | Yes | 1 |

**NOT Gate** <br/>
The NOT Gate inverts the signal of its input, and this can be achieved by having following weights and threshold values:

- Weights **w<sub>1</sub>** and **w<sub>1</sub>** to -1, and
- Threshold **t** = 0

| x<sub>1</sub> | NOT | =>  | Agg. Sum   | is >= Threshold (0) | Output |
| ------------- | --- | --- | ---------- | ------------------- | ------ |
| 0             | 1   | =>  | -1(0) = 0  | Yes                 | 1      |
| 1             | 0   | =>  | -1(1) = -1 | No                  | 0      |

**NAND Gate** <br/>
NAND Gate is a logical composition of the AND GATE followed by NOT Gate. It negates the logic of AND Gate, returning ON(1) when no more than one of its input signal is true, and returns **OFF (0)** when all input signals are true (1). This can be achieved by having the following weights and threshold values:

- Weights **w<sub>1</sub>** and **w<sub>1</sub>** to -1, and
- Threshold **t** = -1

| x<sub>1</sub> | x<sub>2</sub> | NAND | =>  | Agg. Sum        | is >= Threshold (-1) | Output |
| ------------- | ------------- | ---- | --- | --------------- | -------------------- | ------ |
| 0             | 0             | 1    | =>  | -1(0)-1(0) = 0  | Yes                  | 1      |
| 0             | 1             | 1    | =>  | -1(0)-1(1) = -1 | Yes                  | 1      |
| 1             | 0             | 1    | =>  | -1(1)-1(0) = -1 | Yes                  | 1      |
| 1             | 1             | 0    | =>  | -1(1)-(1) = -2  | No                   | 0      |

**NOR Gate** <br/>
NOR Gate is a logical composition of the OR Gate followed by the NOT Gate. It negates the logic of the OR Gate, return **ON (1)** only when none of the inputs are true. This can be achieved by having the following weights and threshold values:

- Weights **w<sub>1</sub>** and **w<sub>1</sub>** to -1, and
- Threshold **t** = 0

| x<sub>1</sub> | x<sub>2</sub> | NOR | =>  | Agg. Sum        | is >= Threshold (0) | Output |
| ------------- | ------------- | --- | --- | --------------- | ------------------- | ------ |
| 0             | 0             | 1   | =>  | -1(0)-1(0) = 0  | Yes                 | 1      |
| 0             | 1             | 0   | =>  | -1(0)-1(1) = -1 | No                  | 0      |
| 1             | 0             | 0   | =>  | -1(1)-1(0) = -1 | No                  | 0      |
| 1             | 1             | 0   | =>  | -1(1)-(1) = -2  | No                  | 0      |

#### 1.1.3 Excitement and Challenges of MCP Neuron

The concept of MCP Neuron seems so simple to represent **artificial intelligence** of any kind, yet it is - and it isn't at the same time. Formal Logic is a fundamental component, and will always remain one of the important ones for the **Computational Intelligence,** and for a machine to have some sort of intelligence it should be able to comprehend logic gates at minimum. The idea being that logic gates can be stringed together to form logic cicruits, capable of executing any kind of instructions.

What makes the MCP Neuron different is that it was able to achieve this through an approach which was inspired by biological neurons, and this was a promising starting point, and there was a LOT and LOT of excitement because of this aspect.

On the other hand if we take a pause and think it through, the main challenge with this concept of **Artificial Neuron** alone is that every logic gate that can be modelled (and hence every logic cirucit which is a collection of neurons could model) had to be pre-programmed i.e., we need to somehow identify and lock the values of **weights** and **threshold** for the neuron to activate certain way based on various input combinations.

This stands out as a massive contrast to how every brain works, which learns from experience. It was not after about a decade or so when various Learning Algorithms were devised, combining these algoritms with the **MCP Neurons**, for the very first time we were able to learn (even through rudimentary stuffs) from the training data, without explictly programming.

In the next section of this blog we will discuss some of these very early learning algorithms, gain an intuitive understanding of how this algorithm works (logically speaking), before we dive deep into the nuts and bolts of the deep learning of today.

And yes, we will also see the limitation which was uncovered in the approach of MCP Neuron + Learning Algorithm, which dried up all the funding and let to the era which is referred to as **AI WINTER**

> On a side note, do you reckon based on all the hype around LLM capabilities and the fact that OpenAI going to incur billion of dollar of losses, are we heading into another AI Winter. If it happens it will be sad, because I do believe there is lot of potential in computation intelligence approaches and LLM/GenAI is just a tiny portion of that.

### 1.2 Learning Algorithms for Aritifical Neurons

It was almost after a decade with the invent of the famour learning algorithms **Perceptron**, that the neural network were able to learn based on the data. But it was not just **perceptron**, which made Neurons mainstream, there are other notable discoveries as well and are listed below:

- [**Hebbian Learning Algorithm**](https://www.historyofinformation.com/detail.php?id=3902), from _Donald Hebb_ in 1949
- [**Perceptron Learning Algorithm**](https://www.historyofinformation.com/detail.php?id=770), from _Frank Rossenblatt_ in 1960, and
- [**Adaline Learning Algorithm**](https://en.wikipedia.org/wiki/ADALINE), from _Bernie Widrow_ in 1962
