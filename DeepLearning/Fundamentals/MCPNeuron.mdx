---
title: "Artificial Neurons"
description: "The fundamental concept of Artificial Neuron as proposed by McCulloch Pitts Neuron"
image: "https://raw.githubusercontent.com/reinventsoni/ssoni-dev-learn-blog/main/_images/DeepLearning/_images/DeepLearningFoundations.webp"
publishedAt: "2024-10-04"
updatedAt: "2024-10-05"
author: "Sanjay Soni"
isPublished: "true"
tags: ["deep-learning", "deep-learning-foundations"]
---

# The First Artificial Neuron

## 1. Historical Roots of Deep Learning

Deep Learning has its root in 1940's, when **Warren McCulloch** and **Warren Pitts** published the concept of Artificial Neuron, also referred to as **MCP Neuron** in 1943 (A Logical Calculus of the Ideas Immanent in Nervous Activity by W.S McCulloch and W.Pitts, Bulletin of Mathetmatical Biophysics, 5(4): 115-133, 193)
[Reference](https://www.historyofinformation.com/detail.php?id=634)
and [Link to Paper](http://www.cse.chalmers.se/~coquand/AUTOMATA/mcp.pdf)

They drew on the three sources to come up with the concept of Artificial Neurons:

- knowledge of basic psychology and functions of neuron brain
- a formal analysis of propositional logic by **Russell** and **Whitehead**, and
- Turing's **Theory of Computation**

### 1.1 McCulloch Pitt's Neuron

The concept of artificial neuron was inspired by the **Biological Neurons** which are interconnected nerve cells in the brain and are involved in the processing and transmitting of various signals, which is illustrated in figure below:

<MDXImage
  src="https://raw.githubusercontent.com/reinventsoni/ssoni-dev-learn-blog/main/_images/DeepLearning/_images/BiologicalNeuron.webp"
  alt="Bilogoical Neuron"
  align="center"
  width="500"
  height="500"
/>

A single neuron in its simplified explanation consists of following parts:

- **SOMA**: the main part of the neuron which processes signal.
- **DENDRITES**: the branch-like shapes which receives signals from other neurons (i.e. are input to neuron), and
- **AXON**: a single nerve which sends signals to other neurons (i.e., is an output of the neuron)

and the connections between the nerurons are known as **SYNAPSES**

The idea of the **McCulloch Pitts Neuron** was to provide the abstractions on how the brain neuron works, and was considered as a simple logic gate which receives

- multiple input binary signals (equivalent to dendrites),
- binary output i.e. **ON** or **OFF** state (i.e. output of neuron, equivalent to Axon),
- and the neruron fires if the accumulates signal of all the inputs have enough stimulation in it, or in other words is above certain threshold.

#### 1.1.1 Formal Mathematical Definition of McCulloch Pitt's Neuron

- The inputs to MCP Neuron can be considered as a vector: **x**: [x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>, ....... x<sub>n-1</sub>, x<sub>n</sub>]
- Every input would have certain weights (importance) associated with it and let that be represented by the vector of weights **w**: [w<sub>1</sub>, w<sub>2</sub>, w<sub>3</sub>, ....... w<sub>n-1</sub>, w<sub>n</sub>], where each **w<sub>i</sub>** has a value of -1, 0 or 1
  - input signal with weight **1** are called excitory input, since they contribute towards a positive output signal in the sum
  - input signal with weight of **-1** are called inhibitory since they repress a positive output signal in the sum
  - input signal with weight of **0** do not contribute at all to the neuron
- The accumulated signal (sometimes also referred to as **net input**) can then be calculated as follows:
  - z = w<sub>1</sub>.x<sub>1</sub> + w<sub>2</sub>.x<sub>2</sub> + .... + w<sub>n-1</sub>.x<sub>n-1</sub> + w<sub>n</sub>.x<sub>n</sub>
- Then for some threshold value **t**, an integer, the output signal is determined by the **decision function** or **activation function** as follows:
  - y = 1, if z >= t or 0 otherwise

The neuron is said to be activated or in **ON** state (i.e. having value of 1), when the weighted sum is greater than the threshold value

<MDXImage
  src="https://raw.githubusercontent.com/reinventsoni/ssoni-dev-learn-blog/main/_images/DeepLearning/_images/BiologicalvsAritificalNeuron.png"
  alt="Biological Neuron vs Aritificial Neuron"
  align="center"
  width="1200"
  height="500"
/>

#### 1.1.2 Original Experiments using McCulloch Pitt's Neuron

The original experiments proposed by **Warren McCulloch** and **Warren Pitts** to use this model of artificial neurons, and construct different **LOGIC GATES** by simply specifying what the **weights** and **threshold** value should be. It can be easily shown that **MCP Neuron** can be used to model the **AND**, **OR** and **NOT** logic gates, as well as the composition of these three logic gates

**OR Gate**<br/>
OR Gate is a logic gate which returns true (1) if atleast one of the input signal is true i.e. 1. This can be achieved by setting

- Weights **w<sub>1</sub>** and **w<sub>1</sub>** to 1, and
- Threshold **t** = 1
  and we can easily see that, whenever the aggregated sum is greater then or equal to threshold t, the output is ON (as expected from OR operation)

| x<sub>1</sub> | x<sub>2</sub> | OR  | =>  | Agg. Sum      | is >= Threshold (1) | Output |
| ------------- | ------------- | --- | --- | ------------- | ------------------- | ------ |
| 0             | 0             | 0   | =>  | 1(0)+1(0) = 0 | No                  | 0      |
| 0             | 1             | 1   | =>  | 1(0)+1(1) = 1 | Yes                 | 1      |
| 1             | 0             | 1   | =>  | 1(1)+1(0) = 1 | Yes                 | 1      |
| 1             | 1             | 1   | =>  | 1(1)+1(1) = 1 | Yes                 | 1      |

**AND Gate**<br/>
AND Gate is a logic gate which returns true (1) if atleast one of the input signal is true i.e. 1. This can be achieved by setting

- Weights **w<sub>1</sub>** and **w<sub>1</sub>** to 1, and
- Threshold **t** = 2
  and we can easily see that, whenever the aggregated sum is greater then or equal to threshold t, the output is ON (as expected from OR operation)
  | x<sub>1</sub> | x<sub>2</sub> | OR | => | Agg. Sum | is >= Threshold (2) | Output |
  | ------------- | ------------- | -- |----| --------------| ------------------ | -------- |
  | 0 | 0 | 0 | => | 1(0)+1(0) = 0 | No | 0 |
  | 0 | 1 | 0 | => | 1(0)+1(1) = 1 | No | 0 |
  | 1 | 0 | 0 | => | 1(1)+1(0) = 1 | No | 0 |
  | 1 | 1 | 1 | => | 1(1)+1(1) = 2 | Yes | 1 |

**NOT Gate** <br/>
The NOT Gate inverts the signal of its input, and this can be achieved by having following weights and threshold values:

- Weights **w<sub>1</sub>** and **w<sub>1</sub>** to -1, and
- Threshold **t** = 0

| x<sub>1</sub> | NOT | =>  | Agg. Sum   | is >= Threshold (0) | Output |
| ------------- | --- | --- | ---------- | ------------------- | ------ |
| 0             | 1   | =>  | -1(0) = 0  | Yes                 | 1      |
| 1             | 0   | =>  | -1(1) = -1 | No                  | 0      |

**NAND Gate** <br/>
NAND Gate is a logical composition of the AND GATE followed by NOT Gate. It negates the logic of AND Gate, returning ON(1) when no more than one of its input signal is true, and returns **OFF (0)** when all input signals are true (1). This can be achieved by having the following weights and threshold values:

- Weights **w<sub>1</sub>** and **w<sub>1</sub>** to -1, and
- Threshold **t** = -1

| x<sub>1</sub> | x<sub>2</sub> | NAND | =>  | Agg. Sum        | is >= Threshold (-1) | Output |
| ------------- | ------------- | ---- | --- | --------------- | -------------------- | ------ |
| 0             | 0             | 1    | =>  | -1(0)-1(0) = 0  | Yes                  | 1      |
| 0             | 1             | 1    | =>  | -1(0)-1(1) = -1 | Yes                  | 1      |
| 1             | 0             | 1    | =>  | -1(1)-1(0) = -1 | Yes                  | 1      |
| 1             | 1             | 0    | =>  | -1(1)-(1) = -2  | No                   | 0      |

**NOR Gate** <br/>
NOR Gate is a logical composition of the OR Gate followed by the NOT Gate. It negates the logic of the OR Gate, return **ON (1)** only when none of the inputs are true. This can be achieved by having the following weights and threshold values:

- Weights **w<sub>1</sub>** and **w<sub>1</sub>** to -1, and
- Threshold **t** = 0

| x<sub>1</sub> | x<sub>2</sub> | NOR | =>  | Agg. Sum        | is >= Threshold (0) | Output |
| ------------- | ------------- | --- | --- | --------------- | ------------------- | ------ |
| 0             | 0             | 1   | =>  | -1(0)-1(0) = 0  | Yes                 | 1      |
| 0             | 1             | 0   | =>  | -1(0)-1(1) = -1 | No                  | 0      |
| 1             | 0             | 0   | =>  | -1(1)-1(0) = -1 | No                  | 0      |
| 1             | 1             | 0   | =>  | -1(1)-(1) = -2  | No                  | 0      |

#### 1.1.3 Excitement and Challenges of MCP Neuron

The concept of MCP Neuron seems so simple to represent **artificial intelligence** of any kind, yet it is - and it isn't at the same time. Formal Logic is a fundamental component, and will always remain one of the important ones for the **Computational Intelligence,** and for a machine to have some sort of intelligence it should be able to comprehend logic gates at minimum. The idea being that logic gates can be stringed together to form logic cicruits, capable of executing any kind of instructions.

What makes the MCP Neuron different is that it was able to achieve this through an approach which was inspired by biological neurons, and this was a promising starting point, and there was a LOT and LOT of excitement because of this aspect.

On the other hand if we take a pause and think it through, the main challenge with this concept of **Artificial Neuron** alone is that every logic gate that can be modelled (and hence every logic cirucit which is a collection of neurons could model) had to be pre-programmed i.e., we need to somehow identify and lock the values of **weights** and **threshold** for the neuron to activate certain way based on various input combinations.

This stands out as a massive contrast to how every brain works, which learns from experience. It was not after about a decade or so when various Learning Algorithms were devised, combining these algoritms with the **MCP Neurons**, for the very first time we were able to learn (even through rudimentary stuffs) from the training data, without explictly programming.

In the next section of this blog we will discuss some of these very early learning algorithms, gain an intuitive understanding of how this algorithm works (logically speaking), before we dive deep into the nuts and bolts of the deep learning of today.

And yes, we will also see the limitation which was uncovered in the approach of MCP Neuron + Learning Algorithm, which dried up all the funding and let to the era which is referred to as **AI WINTER**

> On a side note, do you reckon based on all the hype around LLM capabilities and the fact that OpenAI going to incur billion of dollar of losses, are we heading into another AI Winter. If it happens it will be sad, because I do believe there is lot of potential in computation intelligence approaches and LLM/GenAI is just a tiny portion of that.

#### 1.1.4 Limitations of MCP Neuron

Now we have already see that MCP Neuron experiments were focused on constructing Logical Gates using the weighted sum approach which resembles (atleast at a very high level) how the neurological brain works.

The following figure summarize the workings of MCP Neuron:

<img
  src="https://raw.githubusercontent.com/reinventsoni/ssoni-dev-learn-blog/main/_images/DeepLearning/_images/MCPNeuronThreshold.png"
  align="center"
  width="700"
  height="700"
/>

This was actually a simplest of the binary classification, but there are various limitations in the concept MCP Neuron, and they are as follows:

- it process boolean inputs only.
- it gives equal weights the each inputs
- the threshold value theta (or t) must be chosen manually
- the learning aspect was missing, i.e. it cannot learn from data, already highlighted by the fact that weights and thresholds have to be chosen beforehand manually

For all of these reasons, various Learning Algorithms were proposed in the coming years, which helped push the area of Neural Networks (or the **connectionist approach** of AI)

### 1.2 Learning Algorithms for Aritifical Neurons

It was almost after a decade with the invent of the famour learning algorithms **Perceptron**, that the neural network were able to learn based on the data. But it was not just **perceptron**, which made Neurons mainstream, there are other notable discoveries as well and are listed below:

- [**Hebbian Learning Algorithm**](https://www.historyofinformation.com/detail.php?id=3902), from _Donald Hebb_ in 1949
- [**Perceptron Learning Algorithm**](https://www.historyofinformation.com/detail.php?id=770), from _Frank Rossenblatt_ in 1960, and
- [**Adaline Learning Algorithm**](https://en.wikipedia.org/wiki/ADALINE), from _Bernie Widrow_ in 1962

#### 1.2.1 Rosenblatt's Perceptron Algorithm (1957)

The Rosenblatt's Perceptron Algorithm was designed to overcome most of the issues of McCulloch-Pitt's neuron:

- it can process non-boolean inputs
- it included the learning capabilities, where based on the data it can assign the weights of each input individually,
- it can compute the threshold automatically.

Let's consider the setup of Neuron for Perceptron Algorithm now, where we will adjust the representation slightly my moving the threshold to the left, and including it in the calculation of weighted aggregation. And with the the equation can be adjusted and made simpler as is highlighted in the diagram below:

<img
  src="https://raw.githubusercontent.com/reinventsoni/ssoni-dev-learn-blog/main/_images/DeepLearning/_images/PerceptronNeuron.png"
  align="center"
  width="700"
  height="700"
/>

As we have moved the arbitrary threshold value (theta) to left and included it in the calculation of weighted sum it becomes learnable **(more on the actual algorithm and how the learning happen shortly)**. Here in this setup **w<sub>0</sub>** which is equal to -theta, is called bias, and the weight assigned to this is x<sub>0</sub> and is equal to 1.

Perceptron Algorithm was proposed as a Binary Classification algorithm (i.e. predicting values 0 and 1), so we will now step by step build the algorithm, building our intution algebraically as well as geometrically (obviously for a better understanding)

Let's first have a look at a linearly separable (dummy) dataset with two inputs X1 and X2 just for simplicity, and having two class labels 0 and 1.

Let's say following is our training dataset, based on which we want to train our Artificial Neuron, by the visual inspection of it we can quicky draw few Linear Classifiers through the dataset pretty easily (some of Binary Linear Classifiers are depicted through various lines in the figure on the right hand side

<MDXImage
  src="https://raw.githubusercontent.com/reinventsoni/ssoni-dev-learn-blog/main/_images/DeepLearning/_images/Step0_Visual_Look_at_Data.png"
  align="center"
  height="700"
  width="700"
/>

**CONCEPT 1: Algebric Representation of Line / Parameterizing the Linear Classifier Line**:

Now for us to mathematically find the 'a' linear classifier that can divide this dataset into two, the first step is to paramaterize the **line**, that our Preceptron Algorithm will learn

<MDXImage src="image/Concept1_ParameterizeLine.png" align="center" width="1000" height="1000" />

So as we can see based on the diagram above, the first step of calculation that our Artificial Neuron does represent a line. So by essentially having weight Ws as the parameter associated with each input and threshold value, we have parameterized the line that will represent the Linear Classifier.

**CONCEPT 2: Vector Representation of Line and Geometric/Visual Interpretation**:

In this dummy dataset we have only considered 2 inputs (features) in set, but in practice we seldom have only 2 features to consider, and as such we will using the **Vectors/Matrices** to perform the linear transformations, i.e. taking the weighed sum of the inputs features, and it is prudent to understand visually (geomterically) what happens when we do Matrix Multiplcation and Additions/Subtractions etc.

This visual intution of the operations will be very useful in understanding why the **Perceptron Algorithm**, and for that matter many other algorithms which we will learn down the line works

<MDXImage
  src="https://raw.githubusercontent.com/reinventsoni/ssoni-dev-learn-blog/main/_images/DeepLearning/_images/Concept2_Vector Representation of Line.png"
  align="center"
  height="700"
  width="700"
/>

So in summary:

- Algorithmically, the weighted sum of the inputs and weights, can be performed as Matrix Multiplication,
- And the Vector corresponding to Weights Ws, actually represent a vector orthogonal to the Linear Classifier
- The space on one side of the Linear Classifier will represent +ve space (Class / Label 1) and -ve space on the other (Class Label 0)

> With this mathemtical and vector concepts out of the way, intuitively understanding the Perceptron Algorithm and why it works the way it do will be pretty easy

#### 1.2.1.1 Step1: Initialize the Weights and Bias Unit.

In the original Perceptron Algorithm, weights and bias were initialized to 0. In practice though, in actual implementation we can also initialized these to 0

#### 1.2.1.2 Step1: Iterate through all the Training Example, Perform Calculation

In this step we will now go through each of the training example one by one and perform following operation:

- Compute aggregated sum z = w0x0 + w1x1 + w2x2
- Compute the output value y = 1, if z>=0 and y=0 if z<0
- Update the weight and bias units (simultaneously)
  - w<sub>j</sub> {=} w<sub>j</sub> + (y<sup>(i)</sup> - y_hat<sup>(i)</sup>) . x<sub>j</sub><sup>(i)</sup>
  - b {=} b + (y<sup>(i)</sup> - y_hat<sup>(i)</sup>)
