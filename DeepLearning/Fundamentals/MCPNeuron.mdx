---
title: "Artificial Neurons"
description: "The fundamental concept of Artificial Neuron as proposed by McCulloch Pitts Neuron"
image: "https://raw.githubusercontent.com/reinventsoni/ssoni-dev-learn-blog/main/_images/DeepLearning/_images/DeepLearningFoundations.webp"
publishedAt: "2024-10-04"
updatedAt: "2024-10-05"
author: "Sanjay Soni"
isPublished: "true"
tags: ["deep-learning", "deep-learning-foundations"]
---

## 1. Historical Roots of Deep Learning

Deep Learning has its root in 1940's, when **Warren McCulloch** and **Warren Pitts** published the concept of Artificial Neuron, also referred to as **MCP Neuron** in 1943 (A Logical Calculus of the Ideas Immanent in Nervous Activity by W.S McCulloch and W.Pitts, Bulletin of Mathetmatical Biophysics, 5(4): 115-133, 193)
[Reference](https://www.historyofinformation.com/detail.php?id=634)
and [Link to Paper](http://www.cse.chalmers.se/~coquand/AUTOMATA/mcp.pdf)

They drew on the three sources to come up with the concept of Artificial Neurons:

- knowledge of basic psychology and functions of neuron brain
- a formal analysis of propositional logic by **Russell** and **Whitehead**, and
- Turing's **Theory of Computation**

### 1.1 McCulloch Pitt's Neuron

The concept of artificial neuron was inspired by the **Biological Neurons** which are interconnected nerve cells in the brain and are involved in the processing and transmitting of various signals, which is illustrated in figure below:

<MDXImage
  src="https://raw.githubusercontent.com/reinventsoni/ssoni-dev-learn-blog/main/_images/DeepLearning/_images/BiologicalNeuron.webp"
  alt="Bilogoical Neuron"
  align="center"
  width="500"
  height="500"
/>

A single neuron in its simplified explanation consists of following parts:

- **SOMA**: the main part of the neuron which processes signal.
- **DENDRITES**: the branch-like shapes which receives signals from other neurons (i.e. are input to neuron), and
- **AXON**: a single nerve which sends signals to other neurons (i.e., is an output of the neuron)

and the connections between the nerurons are known as **SYNAPSES**

The idea of the **McCulloch Pitts Neuron** was to provide the abstractions on how the brain neuron works, and was considered as a simple logic gate which receives

- multiple input binary signals (equivalent to dendrites),
- binary output i.e. **ON** or **OFF** state (i.e. output of neuron, equivalent to Axon),
- and the neruron fires if the accumulates signal of all the inputs have enough stimulation in it, or in other words is above certain threshold.

**Formal Mathematical Definition of MCP Neuron**

- The inputs to MCP Neuron can be considered as a vector: **x**: [x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>, ....... x<sub>n-1</sub>, x<sub>n</sub>]
- Every input would have certain weights (importance) associated with it and let that be represented by the vector of weights **w**: [w<sub>1</sub>, w<sub>2</sub>, w<sub>3</sub>, ....... w<sub>n-1</sub>, w<sub>n</sub>], where each **w<sub>i</sub>** has a value of -1, 0 or 1
  - input signal with weight **1** are called excitory input, since they contribute towards a positive output signal in the sum
  - input signal with weight of **-1** are called inhibitory since they repress a positive output signal in the sum
  - input signal with weight of **0** do not contribute at all to the neuron
- The accumulated signal (sometimes also referred to as **net input**) can then be calculated as follows:
  - z = w<sub>1</sub>.x<sub>1</sub> + w<sub>2</sub>.x<sub>2</sub> + .... + w<sub>n-1</sub>.x<sub>n-1</sub> + w<sub>n</sub>.x<sub>n</sub>
- Then for some threshold value **t**, an integer, the output signal is determined by the **decision function** or **activation function** as follows:
  - y = 1, if z >= t or 0 otherwise

<MDXImage
  src="https://raw.githubusercontent.com/reinventsoni/ssoni-dev-learn-blog/main/_images/DeepLearning/_images/BiologicalvsAritificalNeuron.png"
  alt="Biological Neuron vs Aritificial Neuron"
  align="center"
  width="1200"
  height="500"
/>

### 1.2 Learning Algorithms of Early Days

```python
import numpy as np
```
